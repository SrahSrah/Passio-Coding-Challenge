{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Recognition System For Passio\n",
    "## By: Sarah Hernandez\n",
    "\n",
    "### Complied in the Google Cloud Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 1: Prepare a Dataset:\n",
    "\n",
    "#### Step 1: Explore Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foods: 144\n",
      "Not Foods: 125\n",
      "Example Food Shape: (768, 1024, 3)\n",
      "Example Not Food Shape: (4032, 3024, 3)\n",
      "Example Food Shape: (1024, 1024, 3)\n",
      "Example Not Food Shape: (640, 480, 3)\n",
      "Example Food Shape: (640, 640, 3)\n",
      "Example Not Food Shape: (360, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "currentDir = os.getcwd()\n",
    "foodDir = currentDir + \"/Food\"\n",
    "notFoodDir = currentDir + \"/Not Food\"\n",
    "\n",
    "foodList = os.listdir(foodDir)\n",
    "numFoods = len(foodList)\n",
    "\n",
    "notFoodList = os.listdir(notFoodDir)\n",
    "numNotFoods = len(notFoodList)\n",
    "\n",
    "print(\"Foods: \" + str(numFoods))\n",
    "print(\"Not Foods: \" + str(numNotFoods))\n",
    "\n",
    "for i in range(3):\n",
    "    testDir = foodDir + \"/\" + foodList[i]\n",
    "    img = cv2.imread(testDir)\n",
    "    print(\"Example Food Shape: \" + str(img.shape))\n",
    "    testDir2 = notFoodDir + \"/\" + notFoodList[i]\n",
    "    img2 = cv2.imread(testDir2)\n",
    "    print(\"Example Not Food Shape: \" + str(img2.shape))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have fewer than 300 images of food and not food, of various square and rectangular sizes. This is a rather small data set, so we'll augment the data using a few tricks:\n",
    "\n",
    "\n",
    "#### Step 2: Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories Created\n"
     ]
    }
   ],
   "source": [
    "# Get array of directories of food and not food\n",
    "def get_image_dirs(foodList, notFoodList):\n",
    "    \n",
    "    foodDirs = []\n",
    "    for food in foodList:\n",
    "        if not food.startswith('.'):\n",
    "            foodDirs.append(foodDir + \"/\" + food)\n",
    "    \n",
    "    notFoodDirs = []\n",
    "    for notFood in notFoodList:\n",
    "        if not notFood.startswith('.'):\n",
    "            notFoodDirs.append(notFoodDir + \"/\" + notFood)\n",
    "        \n",
    "    \n",
    "    return foodDirs, notFoodDirs\n",
    "        \n",
    "\n",
    "foodDirs, notFoodDirs = get_image_dirs(foodList, notFoodList)\n",
    "\n",
    "print(\"Directories Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food Flipped Images Created\n",
      "Not Food Flipped Images Created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First trick: flip and rotate images\n",
    "for i in range(numFoods):\n",
    "    img = cv2.imread(foodDirs[i])\n",
    "    img2 = np.fliplr(img)\n",
    "    img3 = np.flipud(img)\n",
    "    img4 = np.rot90(img)\n",
    "    cv2.imwrite(foodDir + \"/lr\" + str(i) + \".jpg\", img2)\n",
    "    cv2.imwrite(foodDir + \"/ud\" + str(i) + \".jpg\", img3)\n",
    "    cv2.imwrite(foodDir + \"/rot90\" + str(i) + \".jpg\", img4)\n",
    "    \n",
    "print(\"Food Flipped Images Created\")\n",
    "\n",
    "for i in range(numNotFoods):\n",
    "    img = cv2.imread(notFoodDirs[i])\n",
    "    img2 = np.fliplr(img)\n",
    "    img3 = np.flipud(img)\n",
    "    img4 = np.rot90(img)\n",
    "    cv2.imwrite(notFoodDir + \"/lr\" + str(i) + \".jpg\", img2)\n",
    "    cv2.imwrite(notFoodDir + \"/ud\" + str(i) + \".jpg\", img3)\n",
    "    cv2.imwrite(notFoodDir + \"/rot90\" + str(i) + \".jpg\", img4)\n",
    "    \n",
    "print(\"Not Food Flipped Images Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created\n"
     ]
    }
   ],
   "source": [
    "# Update Food and NotFood Dirs:\n",
    "foodList = os.listdir(foodDir)\n",
    "\n",
    "notFoodList = os.listdir(notFoodDir)\n",
    "\n",
    "foodDirs, notFoodDirs = get_image_dirs(foodList, notFoodList)\n",
    "\n",
    "print(\"Directories created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy Food Created\n",
      "Noisy Not Food Created\n"
     ]
    }
   ],
   "source": [
    "# Second trick: add noise to images:\n",
    "for i in range(len(foodDirs)):\n",
    "    \n",
    "    img = cv2.imread(foodDirs[i])\n",
    "    img = cv2.resize(img,(256,256))\n",
    "    row,col,ch= img.shape\n",
    "    mean = 0\n",
    "    gauss = np.random.normal(mean,30,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = img + gauss\n",
    "    cv2.imwrite(foodDir + \"/noisy\" + str(i) + \".jpg\", noisy)\n",
    "\n",
    "\n",
    "print(\"Noisy Food Created\")\n",
    "\n",
    "for i in range(len(notFoodDirs)):\n",
    "    \n",
    "    img = cv2.imread(notFoodDirs[i])\n",
    "    img = cv2.resize(img,(256,256))\n",
    "    row,col,ch= img.shape\n",
    "    mean = math.ceil(255/2)\n",
    "    gauss = np.random.normal(mean,50,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = img + gauss\n",
    "    cv2.imwrite(notFoodDir + \"/noisy\" + str(i) + \".jpg\", noisy)\n",
    "       \n",
    "print(\"Noisy Not Food Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Foods: 1152\n",
      "Number of Not Foods: 1000\n"
     ]
    }
   ],
   "source": [
    "foodList = os.listdir(foodDir)\n",
    "\n",
    "notFoodList = os.listdir(notFoodDir)\n",
    "\n",
    "foodDirs, notFoodDirs = get_image_dirs(foodList, notFoodList)\n",
    "\n",
    "print(\"Number of Foods: \" + str(len(foodDirs)))\n",
    "print(\"Number of Not Foods: \" + str(len(notFoodDirs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got something to work with! Next, let's generate the dataset:\n",
    "\n",
    "#### Step 3: Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Generated\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "class Generate_Dataset:\n",
    "    \n",
    "    def __init__(self, data_dirs):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.labels = []\n",
    "        self.data_paths = []\n",
    "        self.images = []\n",
    "        \n",
    "        # Now we put all data paths in a single matrix and shuffle it: \n",
    "        self.data_paths = np.concatenate([self.data_dirs[0], self.data_dirs[1]])\n",
    "        shuffle(self.data_paths)\n",
    "        \n",
    "        #Next, generate labels:\n",
    "        for path in self.data_paths:\n",
    "            self.labels.append(self.generate_data_labels(path))\n",
    "            self.images.append(self.get_image(path))\n",
    "        \n",
    "        \n",
    "    # Returns label of specified file\n",
    "    def generate_data_labels(self, directory):\n",
    "        labels = []\n",
    "        # Because we're doing a simple binary classification, we can one-hot-encode here:\n",
    "        if \"Not Food\" in directory:\n",
    "            label = [1, 0]\n",
    "        else:\n",
    "            label = [0, 1]\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    \n",
    "    def get_data_paths(self, startIndex, endIndex):\n",
    "        return self.data_paths[startIndex:endIndex]\n",
    "    \n",
    "    def get_data_labels(self, startIndex, endIndex):\n",
    "        return self.labels[startIndex: endIndex]\n",
    "        \n",
    "        \n",
    "    def get_image(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img,(256,256))\n",
    "        return img\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.images, self.labels\n",
    "    \n",
    "    def get_all_dirs(self):\n",
    "        return self.data_paths\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "dirs = [foodDirs, notFoodDirs]\n",
    "dataset = Generate_Dataset(dirs)\n",
    "images, labels = dataset.get_data()\n",
    "paths = dataset.get_all_dirs()\n",
    "\n",
    "print(\"Dataset Generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we've got an array of images, all scaled down to 256x256, and an array of labels that correspond to the images. But we still need to do some preprocessing:\n",
    "\n",
    "### Step 4: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    #Returns a normalized image, x: input image data in numpy array [256, 256, 3]\n",
    "    \n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_helper(some_images, some_labels, some_paths, filename, isTraining = False):\n",
    "    some_images = normalize(some_images)\n",
    "    #some_images = some_images.reshape((len(some_images), 3, 256, 256))\n",
    "    \n",
    "    num_images = len(some_images)\n",
    "    \n",
    "    if not isTraining:\n",
    "        pickle.dump((some_images, some_labels, some_paths), open(filename, \"wb\"))\n",
    "    else:\n",
    "        # break training images into five batches\n",
    "        for i in range(5):\n",
    "            newFileName = filename + str(i) + \".p\"\n",
    "            first_index = int(num_images*i/5)\n",
    "            second_index = int(num_images*(i+1)/5)\n",
    "            pickle.dump((some_images[first_index:second_index], some_labels[first_index:second_index], some_paths[first_index:second_index]), open(newFileName, \"wb\"))\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "def preprocess(currentDir):\n",
    "    \n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    validation_paths = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    test_paths = []\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    training_paths = []\n",
    "    # Save 10% of data for validation, and another 10% for testing:\n",
    "    first_index = int(len(images)*0.1)\n",
    "    second_index = int(len(images)*0.2)\n",
    "    \n",
    "    \n",
    "    # Save validation set:\n",
    "    validation_images.extend(images[0:first_index])\n",
    "    validation_labels.extend(labels[0:first_index])\n",
    "    validation_paths.extend(paths[0:first_index])\n",
    "    filename = currentDir + \"/\" + \"preprocess_validation.p\"\n",
    "    preprocess_helper(np.array(validation_images), np.array(validation_labels), np.array(validation_paths), filename)\n",
    "    print(\"Validation Set Saved\")\n",
    "    \n",
    "    # Save testing set:\n",
    "    test_images.extend(images[first_index:second_index])\n",
    "    test_labels.extend(labels[first_index:second_index])\n",
    "    test_paths.extend(paths[first_index:second_index])\n",
    "    filename = currentDir + \"/\" + \"preprocess_testing.p\"\n",
    "    preprocess_helper(np.array(test_images), np.array(test_labels), np.array(test_paths), filename)\n",
    "    print(\"Testing Set Saved\")\n",
    "    \n",
    "    # Save training set!\n",
    "    training_images.extend(images[second_index:])\n",
    "    training_labels.extend(labels[second_index:])\n",
    "    training_paths.extend(paths[second_index:])\n",
    "    filename = currentDir + \"/\" + \"preprocess_training\"\n",
    "    preprocess_helper(np.array(training_images), np.array(training_labels), np.array(training_paths), filename, True)\n",
    "    print(\"Training Set Saved\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Saved\n",
      "Testing Set Saved\n",
      "Training Set Saved\n",
      "All sets created and loaded\n"
     ]
    }
   ],
   "source": [
    "preprocess(currentDir)\n",
    "test_images, test_labels, test_paths = pickle.load(open(currentDir + \"/\" + \"preprocess_testing.p\", mode = \"rb\"))\n",
    "valid_images, valid_labels, valid_paths = pickle.load(open(currentDir + \"/\" + \"preprocess_validation.p\", mode = \"rb\"))\n",
    "\n",
    "# Split training images and labels into five batches:\n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_paths = []\n",
    "for i in range(5): \n",
    "    batch_images, batch_labels, batch_paths = pickle.load(open(currentDir + \"/\" + \"preprocess_training\" + str(i) + \".p\", mode = \"rb\"))\n",
    "    train_images.append(batch_images)\n",
    "    train_labels.append(batch_labels)\n",
    "    train_paths.append(batch_paths)\n",
    "    \n",
    "print(\"All sets created and loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 2: Implement a neural network for classifying food vs non-food\n",
    "\n",
    "#### Step 1: Prepare Model \n",
    "We will prepare the model by creating several helper functions. The first of these will help us get mini-batches as needed for training. The remaining are methods that will ech define a layer of the model: a convolutional layer, a flattening layer, a fully connected layer, or the final output layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create methods to get mini-batches\n",
    "def get_mini_batches(batch_size, batch_images, batch_labels, batch_paths):\n",
    "    # Returns images and labels in batches\n",
    "   \n",
    "    for start in range(0, len(batch_images), batch_size):\n",
    "        end = min(start + batch_size, len(batch_images))\n",
    "        \n",
    "        temp_img = list(batch_images[start:end])\n",
    "        \n",
    "        temp_labels = list(batch_labels[start:end])\n",
    "        \n",
    "        temp_paths = list(batch_paths[start:end])\n",
    "        \n",
    "        yield temp_img, temp_labels, temp_paths\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :-param x_tensor: TensorFlow Tensor\n",
    "    :-param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :-param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :-param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    h_in =  int(x_tensor.shape[1])\n",
    "    w_in =  int(x_tensor.shape[2])\n",
    "    h = math.ceil(float(h_in - conv_strides[0] + 1) / float(conv_strides[0]))\n",
    "    w = math.ceil(float(w_in - conv_strides[1] + 1) / float(conv_strides[1]))\n",
    "    \n",
    "    mean = 0.0\n",
    "    stddev = 0.01\n",
    "    weights_init = tf.random_normal([*conv_ksize, int(x_tensor.shape[3]), conv_num_outputs], mean=0.0, stddev=0.01, dtype=tf.float32)\n",
    "    weights = tf.Variable(weights_init)\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    c_strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    p_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    p_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    padding = \"SAME\"\n",
    "    \n",
    "    \n",
    "    conv = tf.nn.conv2d(tf.to_float(x_tensor), weights, c_strides, padding)\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    conv = tf.nn.max_pool(conv, ksize = p_ksize , strides = p_strides, padding = padding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return conv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    shape = x_tensor.get_shape().as_list()        \n",
    "    dim = np.prod(shape[1:])            \n",
    "    x2 = tf.reshape(x_tensor, [-1, dim])          \n",
    "    \n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\" \n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([int(x_tensor.shape[-1]), num_outputs], mean=0.0, stddev=0.01))\n",
    "\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "\n",
    "    layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    stddev = 0.01\n",
    "    weight_init = tf.truncated_normal([int(x_tensor.shape[-1]), num_outputs], mean=mean, stddev= stddev)\n",
    "    weights = tf.Variable(weight_init)\n",
    "\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "\n",
    "\n",
    "    output = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    return output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Build the Model\n",
    "Now, we'll combine the helper function above to create a multi-layered CNN model.\n",
    "The model is laregly based off of prior succesfull image classification models, as shown here:\n",
    "\n",
    "<img src = \"cnn_network.jpg\">\n",
    "\n",
    "\n",
    "Like in the image above, I will start off with a few convolutional layers (followed the addition of bias and the application of max pooling), followed by a flattening layer. Next will be several fully connected layers, increasing in size until the final layer, the output layer, converges into two logit outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = 2\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, 256, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2 = conv2d_maxpool(conv1, 512, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv3 = conv2d_maxpool(conv1, 1024, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    \n",
    "    flat = flatten(conv3)\n",
    "    \n",
    "    \n",
    "    fullycon1 = fully_conn(flat, 256)\n",
    "    fullycon2 = fully_conn(fullycon1, 512)\n",
    "    fullycon3 = fully_conn(fullycon2, 1024)\n",
    "    \n",
    "    dropout = tf.nn.dropout(fullycon3, tf.to_float(keep_prob))\n",
    "    \n",
    "    num_outputs = 2\n",
    "    outputs = output(dropout, num_outputs)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 125\n",
    "keep_probability = .5\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, image_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    transposed_images = np.array(image_batch).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    session.run(optimizer, feed_dict = {\"x:0\":transposed_images, \"y:0\": np.array(label_batch), \"keep_prob:0\": keep_probability})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, image_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    transposed_images = np.array(image_batch).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: transposed_images, y: np.array(label_batch), keep_prob: 1.0})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\"x:0\": transposed_images, \"y:0\": np.array(label_batch), \"keep_prob:0\": 1.0})\n",
    "    \n",
    "    \n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the Model\n",
    "\n",
    "Next, we'll train the model. We'll do so by using the Adam Optimizer for gradient descent, and by shuffling each batch as we train it to increase learning. Here, we'll generate and create the tensorflow graph and run the session in one swift motion.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch Number: 0\n",
      "Epoch 0, Batch 0: \n",
      "Loss:     0.6807 Accuracy: 0.460465\n",
      "Epoch 0, Batch 1: \n",
      "Loss:     0.6791 Accuracy: 0.804651\n",
      "Epoch 0, Batch 2: \n",
      "Loss:     0.5938 Accuracy: 0.618605\n",
      "Epoch 0, Batch 3: \n",
      "Loss:     0.5853 Accuracy: 0.660465\n",
      "Epoch 0, Batch 4: \n",
      "Loss:     0.4902 Accuracy: 0.837209\n",
      "Epoch Number: 1\n",
      "Epoch 1, Batch 0: \n",
      "Loss:     0.5472 Accuracy: 0.688372\n",
      "Epoch 1, Batch 1: \n",
      "Loss:     0.3875 Accuracy: 0.841860\n",
      "Epoch 1, Batch 2: \n",
      "Loss:     0.3462 Accuracy: 0.865116\n",
      "Epoch 1, Batch 3: \n",
      "Loss:     0.3294 Accuracy: 0.860465\n",
      "Epoch 1, Batch 4: \n",
      "Loss:     0.4580 Accuracy: 0.776744\n",
      "Epoch Number: 2\n",
      "Epoch 2, Batch 0: \n",
      "Loss:     0.3373 Accuracy: 0.837209\n",
      "Epoch 2, Batch 1: \n",
      "Loss:     0.3872 Accuracy: 0.841860\n",
      "Epoch 2, Batch 2: \n",
      "Loss:     0.3080 Accuracy: 0.855814\n",
      "Epoch 2, Batch 3: \n",
      "Loss:     0.3254 Accuracy: 0.869767\n",
      "Epoch 2, Batch 4: \n",
      "Loss:     0.4109 Accuracy: 0.823256\n",
      "Epoch Number: 3\n",
      "Epoch 3, Batch 0: \n",
      "Loss:     0.3599 Accuracy: 0.837209\n",
      "Epoch 3, Batch 1: \n",
      "Loss:     0.5133 Accuracy: 0.702326\n",
      "Epoch 3, Batch 2: \n",
      "Loss:     0.4510 Accuracy: 0.786047\n",
      "Epoch 3, Batch 3: \n",
      "Loss:     0.2729 Accuracy: 0.888372\n",
      "Epoch 3, Batch 4: \n",
      "Loss:     0.3531 Accuracy: 0.832558\n",
      "Epoch Number: 4\n",
      "Epoch 4, Batch 0: \n",
      "Loss:     0.3961 Accuracy: 0.800000\n",
      "Epoch 4, Batch 1: \n",
      "Loss:     0.4901 Accuracy: 0.744186\n",
      "Epoch 4, Batch 2: \n",
      "Loss:     0.3442 Accuracy: 0.860465\n",
      "Epoch 4, Batch 3: \n",
      "Loss:     0.3390 Accuracy: 0.855814\n",
      "Epoch 4, Batch 4: \n",
      "Loss:     0.2734 Accuracy: 0.865116\n",
      "Epoch Number: 5\n",
      "Epoch 5, Batch 0: \n",
      "Loss:     0.3111 Accuracy: 0.846512\n",
      "Epoch 5, Batch 1: \n",
      "Loss:     0.3092 Accuracy: 0.869767\n",
      "Epoch 5, Batch 2: \n",
      "Loss:     0.6058 Accuracy: 0.548837\n",
      "Epoch 5, Batch 3: \n",
      "Loss:     0.3391 Accuracy: 0.832558\n",
      "Epoch 5, Batch 4: \n",
      "Loss:     0.2875 Accuracy: 0.883721\n",
      "Epoch Number: 6\n",
      "Epoch 6, Batch 0: \n",
      "Loss:     0.2670 Accuracy: 0.855814\n",
      "Epoch 6, Batch 1: \n",
      "Loss:     0.2822 Accuracy: 0.883721\n",
      "Epoch 6, Batch 2: \n",
      "Loss:     0.2888 Accuracy: 0.869767\n",
      "Epoch 6, Batch 3: \n",
      "Loss:     0.2613 Accuracy: 0.883721\n",
      "Epoch 6, Batch 4: \n",
      "Loss:     0.2784 Accuracy: 0.865116\n",
      "Epoch Number: 7\n",
      "Epoch 7, Batch 0: \n",
      "Loss:     0.2631 Accuracy: 0.883721\n",
      "Epoch 7, Batch 1: \n",
      "Loss:     0.2882 Accuracy: 0.888372\n",
      "Epoch 7, Batch 2: \n",
      "Loss:     0.3000 Accuracy: 0.888372\n",
      "Epoch 7, Batch 3: \n",
      "Loss:     0.2263 Accuracy: 0.883721\n",
      "Epoch 7, Batch 4: \n",
      "Loss:     0.2525 Accuracy: 0.897674\n",
      "Epoch Number: 8\n",
      "Epoch 8, Batch 0: \n",
      "Loss:     0.3138 Accuracy: 0.851163\n",
      "Epoch 8, Batch 1: \n",
      "Loss:     0.2820 Accuracy: 0.855814\n",
      "Epoch 8, Batch 2: \n",
      "Loss:     2.3751 Accuracy: 0.604651\n",
      "Epoch 8, Batch 3: \n",
      "Loss:     0.4320 Accuracy: 0.818605\n",
      "Epoch 8, Batch 4: \n",
      "Loss:     0.4245 Accuracy: 0.813953\n",
      "Epoch Number: 9\n",
      "Epoch 9, Batch 0: \n",
      "Loss:     0.3067 Accuracy: 0.851163\n",
      "Epoch 9, Batch 1: \n",
      "Loss:     0.3360 Accuracy: 0.846512\n",
      "Epoch 9, Batch 2: \n",
      "Loss:     0.3654 Accuracy: 0.832558\n",
      "Epoch 9, Batch 3: \n",
      "Loss:     0.3555 Accuracy: 0.841860\n",
      "Epoch 9, Batch 4: \n",
      "Loss:     0.2701 Accuracy: 0.874419\n",
      "Epoch Number: 10\n",
      "Epoch 10, Batch 0: \n",
      "Loss:     0.3090 Accuracy: 0.874419\n",
      "Epoch 10, Batch 1: \n",
      "Loss:     0.2658 Accuracy: 0.879070\n",
      "Epoch 10, Batch 2: \n",
      "Loss:     0.3061 Accuracy: 0.865116\n",
      "Epoch 10, Batch 3: \n",
      "Loss:     0.2918 Accuracy: 0.855814\n",
      "Epoch 10, Batch 4: \n",
      "Loss:     0.2521 Accuracy: 0.902326\n",
      "Epoch Number: 11\n",
      "Epoch 11, Batch 0: \n",
      "Loss:     0.2526 Accuracy: 0.888372\n",
      "Epoch 11, Batch 1: \n",
      "Loss:     0.2378 Accuracy: 0.869767\n",
      "Epoch 11, Batch 2: \n",
      "Loss:     0.2488 Accuracy: 0.883721\n",
      "Epoch 11, Batch 3: \n",
      "Loss:     0.2381 Accuracy: 0.906977\n",
      "Epoch 11, Batch 4: \n",
      "Loss:     0.2855 Accuracy: 0.893023\n",
      "Epoch Number: 12\n",
      "Epoch 12, Batch 0: \n",
      "Loss:     0.2600 Accuracy: 0.869767\n",
      "Epoch 12, Batch 1: \n",
      "Loss:     0.2685 Accuracy: 0.883721\n",
      "Epoch 12, Batch 2: \n",
      "Loss:     0.4552 Accuracy: 0.883721\n",
      "Epoch 12, Batch 3: \n",
      "Loss:     0.3571 Accuracy: 0.865116\n",
      "Epoch 12, Batch 4: \n",
      "Loss:     0.2606 Accuracy: 0.906977\n",
      "Epoch Number: 13\n",
      "Epoch 13, Batch 0: \n",
      "Loss:     0.2215 Accuracy: 0.902326\n",
      "Epoch 13, Batch 1: \n",
      "Loss:     0.2849 Accuracy: 0.888372\n",
      "Epoch 13, Batch 2: \n",
      "Loss:     0.2477 Accuracy: 0.888372\n",
      "Epoch 13, Batch 3: \n",
      "Loss:     0.2979 Accuracy: 0.883721\n",
      "Epoch 13, Batch 4: \n",
      "Loss:     0.2587 Accuracy: 0.920930\n",
      "Epoch Number: 14\n",
      "Epoch 14, Batch 0: \n",
      "Loss:     0.3781 Accuracy: 0.851163\n",
      "Epoch 14, Batch 1: \n",
      "Loss:     0.2662 Accuracy: 0.888372\n",
      "Epoch 14, Batch 2: \n",
      "Loss:     0.2153 Accuracy: 0.911628\n",
      "Epoch 14, Batch 3: \n",
      "Loss:     0.2298 Accuracy: 0.906977\n",
      "Epoch 14, Batch 4: \n",
      "Loss:     0.2386 Accuracy: 0.916279\n",
      "Epoch Number: 15\n",
      "Epoch 15, Batch 0: \n",
      "Loss:     0.2622 Accuracy: 0.893023\n",
      "Epoch 15, Batch 1: \n",
      "Loss:     0.2321 Accuracy: 0.906977\n",
      "Epoch 15, Batch 2: \n",
      "Loss:     0.2549 Accuracy: 0.906977\n",
      "Epoch 15, Batch 3: \n",
      "Loss:     0.2087 Accuracy: 0.911628\n",
      "Epoch 15, Batch 4: \n",
      "Loss:     0.3612 Accuracy: 0.865116\n",
      "Epoch Number: 16\n",
      "Epoch 16, Batch 0: \n",
      "Loss:     0.2563 Accuracy: 0.874419\n",
      "Epoch 16, Batch 1: \n",
      "Loss:     0.3244 Accuracy: 0.869767\n",
      "Epoch 16, Batch 2: \n",
      "Loss:     0.2397 Accuracy: 0.893023\n",
      "Epoch 16, Batch 3: \n",
      "Loss:     0.2235 Accuracy: 0.925581\n",
      "Epoch 16, Batch 4: \n",
      "Loss:     0.2310 Accuracy: 0.902326\n",
      "Epoch Number: 17\n",
      "Epoch 17, Batch 0: \n",
      "Loss:     0.2448 Accuracy: 0.911628\n",
      "Epoch 17, Batch 1: \n",
      "Loss:     0.3025 Accuracy: 0.888372\n",
      "Epoch 17, Batch 2: \n",
      "Loss:     0.2331 Accuracy: 0.902326\n",
      "Epoch 17, Batch 3: \n",
      "Loss:     0.2708 Accuracy: 0.893023\n",
      "Epoch 17, Batch 4: \n",
      "Loss:     0.2397 Accuracy: 0.893023\n",
      "Epoch Number: 18\n",
      "Epoch 18, Batch 0: \n",
      "Loss:     0.2707 Accuracy: 0.897674\n",
      "Epoch 18, Batch 1: \n",
      "Loss:     0.2685 Accuracy: 0.893023\n",
      "Epoch 18, Batch 2: \n",
      "Loss:     0.2209 Accuracy: 0.916279\n",
      "Epoch 18, Batch 3: \n",
      "Loss:     0.2441 Accuracy: 0.934884\n",
      "Epoch 18, Batch 4: \n",
      "Loss:     0.2657 Accuracy: 0.869767\n",
      "Epoch Number: 19\n",
      "Epoch 19, Batch 0: \n",
      "Loss:     0.2566 Accuracy: 0.897674\n",
      "Epoch 19, Batch 1: \n",
      "Loss:     0.3040 Accuracy: 0.897674\n",
      "Epoch 19, Batch 2: \n",
      "Loss:     0.2454 Accuracy: 0.916279\n",
      "Epoch 19, Batch 3: \n",
      "Loss:     0.2251 Accuracy: 0.916279\n",
      "Epoch 19, Batch 4: \n",
      "Loss:     0.2413 Accuracy: 0.911628\n",
      "Epoch Number: 20\n",
      "Epoch 20, Batch 0: \n",
      "Loss:     0.3752 Accuracy: 0.865116\n",
      "Epoch 20, Batch 1: \n",
      "Loss:     0.4064 Accuracy: 0.906977\n",
      "Epoch 20, Batch 2: \n",
      "Loss:     0.3179 Accuracy: 0.902326\n",
      "Epoch 20, Batch 3: \n",
      "Loss:     0.3142 Accuracy: 0.860465\n",
      "Epoch 20, Batch 4: \n",
      "Loss:     0.2450 Accuracy: 0.906977\n",
      "Epoch Number: 21\n",
      "Epoch 21, Batch 0: \n",
      "Loss:     0.3358 Accuracy: 0.874419\n",
      "Epoch 21, Batch 1: \n",
      "Loss:     0.2201 Accuracy: 0.906977\n",
      "Epoch 21, Batch 2: \n",
      "Loss:     0.2332 Accuracy: 0.930233\n",
      "Epoch 21, Batch 3: \n",
      "Loss:     0.2312 Accuracy: 0.888372\n",
      "Epoch 21, Batch 4: \n",
      "Loss:     0.3021 Accuracy: 0.897674\n",
      "Epoch Number: 22\n",
      "Epoch 22, Batch 0: \n",
      "Loss:     0.2453 Accuracy: 0.897674\n",
      "Epoch 22, Batch 1: \n",
      "Loss:     0.1922 Accuracy: 0.934884\n",
      "Epoch 22, Batch 2: \n",
      "Loss:     0.1954 Accuracy: 0.934884\n",
      "Epoch 22, Batch 3: \n",
      "Loss:     0.2187 Accuracy: 0.944186\n",
      "Epoch 22, Batch 4: \n",
      "Loss:     0.3704 Accuracy: 0.874419\n",
      "Epoch Number: 23\n",
      "Epoch 23, Batch 0: \n",
      "Loss:     0.7545 Accuracy: 0.883721\n",
      "Epoch 23, Batch 1: \n",
      "Loss:     0.3112 Accuracy: 0.851163\n",
      "Epoch 23, Batch 2: \n",
      "Loss:     0.2281 Accuracy: 0.925581\n",
      "Epoch 23, Batch 3: \n",
      "Loss:     0.3374 Accuracy: 0.934884\n",
      "Epoch 23, Batch 4: \n",
      "Loss:     0.2202 Accuracy: 0.888372\n",
      "Epoch Number: 24\n",
      "Epoch 24, Batch 0: \n",
      "Loss:     0.2792 Accuracy: 0.911628\n",
      "Epoch 24, Batch 1: \n",
      "Loss:     0.3020 Accuracy: 0.897674\n",
      "Epoch 24, Batch 2: \n",
      "Loss:     0.2552 Accuracy: 0.944186\n",
      "Epoch 24, Batch 3: \n",
      "Loss:     0.2982 Accuracy: 0.911628\n",
      "Epoch 24, Batch 4: \n",
      "Loss:     0.3849 Accuracy: 0.888372\n",
      "Epoch Number: 25\n",
      "Epoch 25, Batch 0: \n",
      "Loss:     0.3182 Accuracy: 0.902326\n",
      "Epoch 25, Batch 1: \n",
      "Loss:     0.4059 Accuracy: 0.902326\n",
      "Epoch 25, Batch 2: \n",
      "Loss:     0.3054 Accuracy: 0.925581\n",
      "Epoch 25, Batch 3: \n",
      "Loss:     0.2552 Accuracy: 0.897674\n",
      "Epoch 25, Batch 4: \n",
      "Loss:     0.2913 Accuracy: 0.911628\n",
      "Epoch Number: 26\n",
      "Epoch 26, Batch 0: \n",
      "Loss:     0.2402 Accuracy: 0.902326\n",
      "Epoch 26, Batch 1: \n",
      "Loss:     0.2726 Accuracy: 0.916279\n",
      "Epoch 26, Batch 2: \n",
      "Loss:     0.2459 Accuracy: 0.911628\n",
      "Epoch 26, Batch 3: \n",
      "Loss:     0.3474 Accuracy: 0.920930\n",
      "Epoch 26, Batch 4: \n",
      "Loss:     0.3827 Accuracy: 0.897674\n",
      "Epoch Number: 27\n",
      "Epoch 27, Batch 0: \n",
      "Loss:     0.4583 Accuracy: 0.846512\n",
      "Epoch 27, Batch 1: \n",
      "Loss:     0.3169 Accuracy: 0.925581\n",
      "Epoch 27, Batch 2: \n",
      "Loss:     0.1770 Accuracy: 0.916279\n",
      "Epoch 27, Batch 3: \n",
      "Loss:     0.4149 Accuracy: 0.916279\n",
      "Epoch 27, Batch 4: \n",
      "Loss:     0.2469 Accuracy: 0.911628\n",
      "Epoch Number: 28\n",
      "Epoch 28, Batch 0: \n",
      "Loss:     0.3696 Accuracy: 0.911628\n",
      "Epoch 28, Batch 1: \n",
      "Loss:     0.2879 Accuracy: 0.906977\n",
      "Epoch 28, Batch 2: \n",
      "Loss:     0.3327 Accuracy: 0.930233\n",
      "Epoch 28, Batch 3: \n",
      "Loss:     0.2654 Accuracy: 0.920930\n",
      "Epoch 28, Batch 4: \n",
      "Loss:     0.2968 Accuracy: 0.939535\n",
      "Epoch Number: 29\n",
      "Epoch 29, Batch 0: \n",
      "Loss:     0.2039 Accuracy: 0.944186\n",
      "Epoch 29, Batch 1: \n",
      "Loss:     0.1878 Accuracy: 0.925581\n",
      "Epoch 29, Batch 2: \n",
      "Loss:     0.1798 Accuracy: 0.948837\n",
      "Epoch 29, Batch 3: \n",
      "Loss:     0.3496 Accuracy: 0.944186\n",
      "Epoch 29, Batch 4: \n",
      "Loss:     0.3034 Accuracy: 0.902326\n",
      "Epoch Number: 30\n",
      "Epoch 30, Batch 0: \n",
      "Loss:     0.2745 Accuracy: 0.930233\n",
      "Epoch 30, Batch 1: \n",
      "Loss:     0.2296 Accuracy: 0.930233\n",
      "Epoch 30, Batch 2: \n",
      "Loss:     0.1637 Accuracy: 0.948837\n",
      "Epoch 30, Batch 3: \n",
      "Loss:     0.2211 Accuracy: 0.948837\n",
      "Epoch 30, Batch 4: \n",
      "Loss:     0.2207 Accuracy: 0.948837\n",
      "Epoch Number: 31\n",
      "Epoch 31, Batch 0: \n",
      "Loss:     0.2539 Accuracy: 0.916279\n",
      "Epoch 31, Batch 1: \n",
      "Loss:     0.2309 Accuracy: 0.911628\n",
      "Epoch 31, Batch 2: \n",
      "Loss:     0.1960 Accuracy: 0.930233\n",
      "Epoch 31, Batch 3: \n",
      "Loss:     0.1660 Accuracy: 0.953488\n",
      "Epoch 31, Batch 4: \n",
      "Loss:     0.2020 Accuracy: 0.934884\n",
      "Epoch Number: 32\n",
      "Epoch 32, Batch 0: \n",
      "Loss:     0.2588 Accuracy: 0.944186\n",
      "Epoch 32, Batch 1: \n",
      "Loss:     0.3699 Accuracy: 0.934884\n",
      "Epoch 32, Batch 2: \n",
      "Loss:     0.2672 Accuracy: 0.953488\n",
      "Epoch 32, Batch 3: \n",
      "Loss:     0.3084 Accuracy: 0.958140\n",
      "Epoch 32, Batch 4: \n",
      "Loss:     0.4670 Accuracy: 0.893023\n",
      "Epoch Number: 33\n",
      "Epoch 33, Batch 0: \n",
      "Loss:     2.3302 Accuracy: 0.832558\n",
      "Epoch 33, Batch 1: \n",
      "Loss:     0.5536 Accuracy: 0.702326\n",
      "Epoch 33, Batch 2: \n",
      "Loss:     0.5205 Accuracy: 0.711628\n",
      "Epoch 33, Batch 3: \n",
      "Loss:     0.5239 Accuracy: 0.706977\n",
      "Epoch 33, Batch 4: \n",
      "Loss:     0.4649 Accuracy: 0.776744\n",
      "Epoch Number: 34\n",
      "Epoch 34, Batch 0: \n",
      "Loss:     0.4516 Accuracy: 0.772093\n",
      "Epoch 34, Batch 1: \n",
      "Loss:     0.4279 Accuracy: 0.786047\n",
      "Epoch 34, Batch 2: \n",
      "Loss:     0.3886 Accuracy: 0.837209\n",
      "Epoch 34, Batch 3: \n",
      "Loss:     0.3157 Accuracy: 0.855814\n",
      "Epoch 34, Batch 4: \n",
      "Loss:     0.2706 Accuracy: 0.888372\n",
      "Epoch Number: 35\n",
      "Epoch 35, Batch 0: \n",
      "Loss:     0.3596 Accuracy: 0.888372\n",
      "Epoch 35, Batch 1: \n",
      "Loss:     0.2702 Accuracy: 0.902326\n",
      "Epoch 35, Batch 2: \n",
      "Loss:     0.2357 Accuracy: 0.930233\n",
      "Epoch 35, Batch 3: \n",
      "Loss:     0.3069 Accuracy: 0.920930\n",
      "Epoch 35, Batch 4: \n",
      "Loss:     0.2663 Accuracy: 0.953488\n",
      "Epoch Number: 36\n",
      "Epoch 36, Batch 0: \n",
      "Loss:     0.3707 Accuracy: 0.911628\n",
      "Epoch 36, Batch 1: \n",
      "Loss:     0.3556 Accuracy: 0.934884\n",
      "Epoch 36, Batch 2: \n",
      "Loss:     0.3403 Accuracy: 0.920930\n",
      "Epoch 36, Batch 3: \n",
      "Loss:     0.3379 Accuracy: 0.916279\n",
      "Epoch 36, Batch 4: \n",
      "Loss:     0.4710 Accuracy: 0.911628\n",
      "Epoch Number: 37\n",
      "Epoch 37, Batch 0: \n",
      "Loss:     0.3905 Accuracy: 0.930233\n",
      "Epoch 37, Batch 1: \n",
      "Loss:     0.5770 Accuracy: 0.860465\n",
      "Epoch 37, Batch 2: \n",
      "Loss:     0.2415 Accuracy: 0.939535\n",
      "Epoch 37, Batch 3: \n",
      "Loss:     0.2485 Accuracy: 0.948837\n",
      "Epoch 37, Batch 4: \n",
      "Loss:     0.2794 Accuracy: 0.939535\n",
      "Epoch Number: 38\n",
      "Epoch 38, Batch 0: \n",
      "Loss:     0.3799 Accuracy: 0.911628\n",
      "Epoch 38, Batch 1: \n",
      "Loss:     0.2574 Accuracy: 0.939535\n",
      "Epoch 38, Batch 2: \n",
      "Loss:     0.2528 Accuracy: 0.934884\n",
      "Epoch 38, Batch 3: \n",
      "Loss:     0.1997 Accuracy: 0.962791\n",
      "Epoch 38, Batch 4: \n",
      "Loss:     0.2515 Accuracy: 0.944186\n",
      "Epoch Number: 39\n",
      "Epoch 39, Batch 0: \n",
      "Loss:     0.3510 Accuracy: 0.916279\n",
      "Epoch 39, Batch 1: \n",
      "Loss:     0.2926 Accuracy: 0.930233\n",
      "Epoch 39, Batch 2: \n",
      "Loss:     0.2098 Accuracy: 0.934884\n",
      "Epoch 39, Batch 3: \n",
      "Loss:     0.2298 Accuracy: 0.925581\n",
      "Epoch 39, Batch 4: \n",
      "Loss:     0.2227 Accuracy: 0.934884\n",
      "Epoch Number: 40\n",
      "Epoch 40, Batch 0: \n",
      "Loss:     0.2941 Accuracy: 0.925581\n",
      "Epoch 40, Batch 1: \n",
      "Loss:     0.2550 Accuracy: 0.939535\n",
      "Epoch 40, Batch 2: \n",
      "Loss:     0.1914 Accuracy: 0.934884\n",
      "Epoch 40, Batch 3: \n",
      "Loss:     0.2208 Accuracy: 0.958140\n",
      "Epoch 40, Batch 4: \n",
      "Loss:     0.2569 Accuracy: 0.948837\n",
      "Epoch Number: 41\n",
      "Epoch 41, Batch 0: \n",
      "Loss:     0.3169 Accuracy: 0.902326\n",
      "Epoch 41, Batch 1: \n",
      "Loss:     0.3237 Accuracy: 0.925581\n",
      "Epoch 41, Batch 2: \n",
      "Loss:     0.4862 Accuracy: 0.920930\n",
      "Epoch 41, Batch 3: \n",
      "Loss:     0.2128 Accuracy: 0.925581\n",
      "Epoch 41, Batch 4: \n",
      "Loss:     0.1582 Accuracy: 0.958140\n",
      "Epoch Number: 42\n",
      "Epoch 42, Batch 0: \n",
      "Loss:     0.1703 Accuracy: 0.953488\n",
      "Epoch 42, Batch 1: \n",
      "Loss:     0.1983 Accuracy: 0.953488\n",
      "Epoch 42, Batch 2: \n",
      "Loss:     0.2045 Accuracy: 0.939535\n",
      "Epoch 42, Batch 3: \n",
      "Loss:     0.2302 Accuracy: 0.925581\n",
      "Epoch 42, Batch 4: \n",
      "Loss:     0.1981 Accuracy: 0.930233\n",
      "Epoch Number: 43\n",
      "Epoch 43, Batch 0: \n",
      "Loss:     0.2726 Accuracy: 0.930233\n",
      "Epoch 43, Batch 1: \n",
      "Loss:     0.2631 Accuracy: 0.948837\n",
      "Epoch 43, Batch 2: \n",
      "Loss:     0.2505 Accuracy: 0.939535\n",
      "Epoch 43, Batch 3: \n",
      "Loss:     0.2325 Accuracy: 0.953488\n",
      "Epoch 43, Batch 4: \n",
      "Loss:     0.3075 Accuracy: 0.911628\n",
      "Epoch Number: 44\n",
      "Epoch 44, Batch 0: \n",
      "Loss:     0.3368 Accuracy: 0.916279\n",
      "Epoch 44, Batch 1: \n",
      "Loss:     0.2411 Accuracy: 0.930233\n",
      "Epoch 44, Batch 2: \n",
      "Loss:     0.3628 Accuracy: 0.939535\n",
      "Epoch 44, Batch 3: \n",
      "Loss:     0.2218 Accuracy: 0.934884\n",
      "Epoch 44, Batch 4: \n",
      "Loss:     0.1932 Accuracy: 0.944186\n",
      "Epoch Number: 45\n",
      "Epoch 45, Batch 0: \n",
      "Loss:     0.1429 Accuracy: 0.958140\n",
      "Epoch 45, Batch 1: \n",
      "Loss:     0.2420 Accuracy: 0.939535\n",
      "Epoch 45, Batch 2: \n",
      "Loss:     0.4523 Accuracy: 0.911628\n",
      "Epoch 45, Batch 3: \n",
      "Loss:     0.2500 Accuracy: 0.958140\n",
      "Epoch 45, Batch 4: \n",
      "Loss:     0.4512 Accuracy: 0.916279\n",
      "Epoch Number: 46\n",
      "Epoch 46, Batch 0: \n",
      "Loss:     0.6088 Accuracy: 0.841860\n",
      "Epoch 46, Batch 1: \n",
      "Loss:     0.2631 Accuracy: 0.953488\n",
      "Epoch 46, Batch 2: \n",
      "Loss:     0.2107 Accuracy: 0.958140\n",
      "Epoch 46, Batch 3: \n",
      "Loss:     0.2349 Accuracy: 0.934884\n",
      "Epoch 46, Batch 4: \n",
      "Loss:     0.2344 Accuracy: 0.953488\n",
      "Epoch Number: 47\n",
      "Epoch 47, Batch 0: \n",
      "Loss:     0.2173 Accuracy: 0.958140\n",
      "Epoch 47, Batch 1: \n",
      "Loss:     0.2190 Accuracy: 0.953488\n",
      "Epoch 47, Batch 2: \n",
      "Loss:     0.1830 Accuracy: 0.944186\n",
      "Epoch 47, Batch 3: \n",
      "Loss:     0.2256 Accuracy: 0.953488\n",
      "Epoch 47, Batch 4: \n",
      "Loss:     0.2833 Accuracy: 0.939535\n",
      "Epoch Number: 48\n",
      "Epoch 48, Batch 0: \n",
      "Loss:     0.1778 Accuracy: 0.953488\n",
      "Epoch 48, Batch 1: \n",
      "Loss:     0.1340 Accuracy: 0.962791\n",
      "Epoch 48, Batch 2: \n",
      "Loss:     0.1449 Accuracy: 0.962791\n",
      "Epoch 48, Batch 3: \n",
      "Loss:     0.2180 Accuracy: 0.953488\n",
      "Epoch 48, Batch 4: \n",
      "Loss:     0.3785 Accuracy: 0.934884\n",
      "Epoch Number: 49\n",
      "Epoch 49, Batch 0: \n",
      "Loss:     0.2368 Accuracy: 0.944186\n",
      "Epoch 49, Batch 1: \n",
      "Loss:     0.2491 Accuracy: 0.953488\n",
      "Epoch 49, Batch 2: \n",
      "Loss:     0.1399 Accuracy: 0.934884\n",
      "Epoch 49, Batch 3: \n",
      "Loss:     0.2133 Accuracy: 0.944186\n",
      "Epoch 49, Batch 4: \n",
      "Loss:     0.1558 Accuracy: 0.944186\n",
      "Epoch Number: 50\n",
      "Epoch 50, Batch 0: \n",
      "Loss:     0.1980 Accuracy: 0.944186\n",
      "Epoch 50, Batch 1: \n",
      "Loss:     0.1844 Accuracy: 0.948837\n",
      "Epoch 50, Batch 2: \n",
      "Loss:     0.2020 Accuracy: 0.948837\n",
      "Epoch 50, Batch 3: \n",
      "Loss:     0.4052 Accuracy: 0.902326\n",
      "Epoch 50, Batch 4: \n",
      "Loss:     0.3898 Accuracy: 0.809302\n",
      "Epoch Number: 51\n",
      "Epoch 51, Batch 0: \n",
      "Loss:     0.6018 Accuracy: 0.800000\n",
      "Epoch 51, Batch 1: \n",
      "Loss:     0.4207 Accuracy: 0.869767\n",
      "Epoch 51, Batch 2: \n",
      "Loss:     0.2397 Accuracy: 0.883721\n",
      "Epoch 51, Batch 3: \n",
      "Loss:     0.3317 Accuracy: 0.897674\n",
      "Epoch 51, Batch 4: \n",
      "Loss:     0.2998 Accuracy: 0.888372\n",
      "Epoch Number: 52\n",
      "Epoch 52, Batch 0: \n",
      "Loss:     0.2112 Accuracy: 0.916279\n",
      "Epoch 52, Batch 1: \n",
      "Loss:     0.3054 Accuracy: 0.893023\n",
      "Epoch 52, Batch 2: \n",
      "Loss:     0.1961 Accuracy: 0.920930\n",
      "Epoch 52, Batch 3: \n",
      "Loss:     0.2663 Accuracy: 0.911628\n",
      "Epoch 52, Batch 4: \n",
      "Loss:     0.2870 Accuracy: 0.925581\n",
      "Epoch Number: 53\n",
      "Epoch 53, Batch 0: \n",
      "Loss:     0.2508 Accuracy: 0.934884\n",
      "Epoch 53, Batch 1: \n",
      "Loss:     0.2463 Accuracy: 0.925581\n",
      "Epoch 53, Batch 2: \n",
      "Loss:     0.2467 Accuracy: 0.944186\n",
      "Epoch 53, Batch 3: \n",
      "Loss:     0.2414 Accuracy: 0.916279\n",
      "Epoch 53, Batch 4: \n",
      "Loss:     0.4022 Accuracy: 0.939535\n",
      "Epoch Number: 54\n",
      "Epoch 54, Batch 0: \n",
      "Loss:     0.2510 Accuracy: 0.944186\n",
      "Epoch 54, Batch 1: \n",
      "Loss:     0.1928 Accuracy: 0.934884\n",
      "Epoch 54, Batch 2: \n",
      "Loss:     0.3784 Accuracy: 0.911628\n",
      "Epoch 54, Batch 3: \n",
      "Loss:     0.3736 Accuracy: 0.897674\n",
      "Epoch 54, Batch 4: \n",
      "Loss:     0.3613 Accuracy: 0.953488\n",
      "Epoch Number: 55\n",
      "Epoch 55, Batch 0: \n",
      "Loss:     0.4541 Accuracy: 0.934884\n",
      "Epoch 55, Batch 1: \n",
      "Loss:     0.2323 Accuracy: 0.934884\n",
      "Epoch 55, Batch 2: \n",
      "Loss:     0.2362 Accuracy: 0.948837\n",
      "Epoch 55, Batch 3: \n",
      "Loss:     0.3191 Accuracy: 0.920930\n",
      "Epoch 55, Batch 4: \n",
      "Loss:     0.3481 Accuracy: 0.939535\n",
      "Epoch Number: 56\n",
      "Epoch 56, Batch 0: \n",
      "Loss:     0.8660 Accuracy: 0.888372\n",
      "Epoch 56, Batch 1: \n",
      "Loss:     0.2202 Accuracy: 0.934884\n",
      "Epoch 56, Batch 2: \n",
      "Loss:     0.1693 Accuracy: 0.930233\n",
      "Epoch 56, Batch 3: \n",
      "Loss:     0.6013 Accuracy: 0.916279\n",
      "Epoch 56, Batch 4: \n",
      "Loss:     0.2169 Accuracy: 0.930233\n",
      "Epoch Number: 57\n",
      "Epoch 57, Batch 0: \n",
      "Loss:     0.2485 Accuracy: 0.948837\n",
      "Epoch 57, Batch 1: \n",
      "Loss:     0.2368 Accuracy: 0.962791\n",
      "Epoch 57, Batch 2: \n",
      "Loss:     0.3511 Accuracy: 0.958140\n",
      "Epoch 57, Batch 3: \n",
      "Loss:     0.2719 Accuracy: 0.930233\n",
      "Epoch 57, Batch 4: \n",
      "Loss:     0.3863 Accuracy: 0.953488\n",
      "Epoch Number: 58\n",
      "Epoch 58, Batch 0: \n",
      "Loss:     0.3608 Accuracy: 0.944186\n",
      "Epoch 58, Batch 1: \n",
      "Loss:     0.2896 Accuracy: 0.948837\n",
      "Epoch 58, Batch 2: \n",
      "Loss:     0.2592 Accuracy: 0.953488\n",
      "Epoch 58, Batch 3: \n",
      "Loss:     0.3868 Accuracy: 0.948837\n",
      "Epoch 58, Batch 4: \n",
      "Loss:     0.4212 Accuracy: 0.953488\n",
      "Epoch Number: 59\n",
      "Epoch 59, Batch 0: \n",
      "Loss:     0.3768 Accuracy: 0.930233\n",
      "Epoch 59, Batch 1: \n",
      "Loss:     0.4117 Accuracy: 0.939535\n",
      "Epoch 59, Batch 2: \n",
      "Loss:     0.3371 Accuracy: 0.944186\n",
      "Epoch 59, Batch 3: \n",
      "Loss:     0.3110 Accuracy: 0.962791\n",
      "Epoch 59, Batch 4: \n",
      "Loss:     0.3508 Accuracy: 0.944186\n",
      "Epoch Number: 60\n",
      "Epoch 60, Batch 0: \n",
      "Loss:     0.3979 Accuracy: 0.930233\n",
      "Epoch 60, Batch 1: \n",
      "Loss:     0.3293 Accuracy: 0.953488\n",
      "Epoch 60, Batch 2: \n",
      "Loss:     0.2709 Accuracy: 0.948837\n",
      "Epoch 60, Batch 3: \n",
      "Loss:     0.2694 Accuracy: 0.944186\n",
      "Epoch 60, Batch 4: \n",
      "Loss:     0.2904 Accuracy: 0.939535\n",
      "Epoch Number: 61\n",
      "Epoch 61, Batch 0: \n",
      "Loss:     0.2579 Accuracy: 0.944186\n",
      "Epoch 61, Batch 1: \n",
      "Loss:     0.2038 Accuracy: 0.958140\n",
      "Epoch 61, Batch 2: \n",
      "Loss:     0.2115 Accuracy: 0.958140\n",
      "Epoch 61, Batch 3: \n",
      "Loss:     0.2034 Accuracy: 0.958140\n",
      "Epoch 61, Batch 4: \n",
      "Loss:     0.2935 Accuracy: 0.953488\n",
      "Epoch Number: 62\n",
      "Epoch 62, Batch 0: \n",
      "Loss:     0.4342 Accuracy: 0.962791\n",
      "Epoch 62, Batch 1: \n",
      "Loss:     0.2803 Accuracy: 0.948837\n",
      "Epoch 62, Batch 2: \n",
      "Loss:     0.2357 Accuracy: 0.953488\n",
      "Epoch 62, Batch 3: \n",
      "Loss:     0.2941 Accuracy: 0.953488\n",
      "Epoch 62, Batch 4: \n",
      "Loss:     0.3100 Accuracy: 0.939535\n",
      "Epoch Number: 63\n",
      "Epoch 63, Batch 0: \n",
      "Loss:     0.3674 Accuracy: 0.948837\n",
      "Epoch 63, Batch 1: \n",
      "Loss:     0.3590 Accuracy: 0.962791\n",
      "Epoch 63, Batch 2: \n",
      "Loss:     0.4243 Accuracy: 0.953488\n",
      "Epoch 63, Batch 3: \n",
      "Loss:     0.6699 Accuracy: 0.869767\n",
      "Epoch 63, Batch 4: \n",
      "Loss:     0.3654 Accuracy: 0.934884\n",
      "Epoch Number: 64\n",
      "Epoch 64, Batch 0: \n",
      "Loss:     0.2628 Accuracy: 0.934884\n",
      "Epoch 64, Batch 1: \n",
      "Loss:     0.1699 Accuracy: 0.930233\n",
      "Epoch 64, Batch 2: \n",
      "Loss:     0.2107 Accuracy: 0.944186\n",
      "Epoch 64, Batch 3: \n",
      "Loss:     0.3443 Accuracy: 0.934884\n",
      "Epoch 64, Batch 4: \n",
      "Loss:     0.1497 Accuracy: 0.953488\n",
      "Epoch Number: 65\n",
      "Epoch 65, Batch 0: \n",
      "Loss:     0.2117 Accuracy: 0.948837\n",
      "Epoch 65, Batch 1: \n",
      "Loss:     0.3474 Accuracy: 0.948837\n",
      "Epoch 65, Batch 2: \n",
      "Loss:     0.1992 Accuracy: 0.934884\n",
      "Epoch 65, Batch 3: \n",
      "Loss:     0.2240 Accuracy: 0.958140\n",
      "Epoch 65, Batch 4: \n",
      "Loss:     0.2229 Accuracy: 0.953488\n",
      "Epoch Number: 66\n",
      "Epoch 66, Batch 0: \n",
      "Loss:     0.2373 Accuracy: 0.953488\n",
      "Epoch 66, Batch 1: \n",
      "Loss:     0.3188 Accuracy: 0.944186\n",
      "Epoch 66, Batch 2: \n",
      "Loss:     0.3149 Accuracy: 0.958140\n",
      "Epoch 66, Batch 3: \n",
      "Loss:     0.3437 Accuracy: 0.948837\n",
      "Epoch 66, Batch 4: \n",
      "Loss:     0.3797 Accuracy: 0.953488\n",
      "Epoch Number: 67\n",
      "Epoch 67, Batch 0: \n",
      "Loss:     0.3467 Accuracy: 0.962791\n",
      "Epoch 67, Batch 1: \n",
      "Loss:     0.3228 Accuracy: 0.967442\n",
      "Epoch 67, Batch 2: \n",
      "Loss:     0.3959 Accuracy: 0.958140\n",
      "Epoch 67, Batch 3: \n",
      "Loss:     0.4809 Accuracy: 0.944186\n",
      "Epoch 67, Batch 4: \n",
      "Loss:     0.5338 Accuracy: 0.920930\n",
      "Epoch Number: 68\n",
      "Epoch 68, Batch 0: \n",
      "Loss:     0.7027 Accuracy: 0.934884\n",
      "Epoch 68, Batch 1: \n",
      "Loss:     0.7062 Accuracy: 0.846512\n",
      "Epoch 68, Batch 2: \n",
      "Loss:     0.8437 Accuracy: 0.883721\n",
      "Epoch 68, Batch 3: \n",
      "Loss:     0.2314 Accuracy: 0.897674\n",
      "Epoch 68, Batch 4: \n",
      "Loss:     0.2648 Accuracy: 0.925581\n",
      "Epoch Number: 69\n",
      "Epoch 69, Batch 0: \n",
      "Loss:     0.2976 Accuracy: 0.902326\n",
      "Epoch 69, Batch 1: \n",
      "Loss:     0.2541 Accuracy: 0.925581\n",
      "Epoch 69, Batch 2: \n",
      "Loss:     0.2758 Accuracy: 0.934884\n",
      "Epoch 69, Batch 3: \n",
      "Loss:     0.2810 Accuracy: 0.939535\n",
      "Epoch 69, Batch 4: \n",
      "Loss:     0.2626 Accuracy: 0.911628\n",
      "Epoch Number: 70\n",
      "Epoch 70, Batch 0: \n",
      "Loss:     0.2128 Accuracy: 0.944186\n",
      "Epoch 70, Batch 1: \n",
      "Loss:     0.2118 Accuracy: 0.948837\n",
      "Epoch 70, Batch 2: \n",
      "Loss:     0.2658 Accuracy: 0.948837\n",
      "Epoch 70, Batch 3: \n",
      "Loss:     0.3105 Accuracy: 0.958140\n",
      "Epoch 70, Batch 4: \n",
      "Loss:     0.2980 Accuracy: 0.934884\n",
      "Epoch Number: 71\n",
      "Epoch 71, Batch 0: \n",
      "Loss:     0.2794 Accuracy: 0.930233\n",
      "Epoch 71, Batch 1: \n",
      "Loss:     0.3128 Accuracy: 0.948837\n",
      "Epoch 71, Batch 2: \n",
      "Loss:     0.4253 Accuracy: 0.916279\n",
      "Epoch 71, Batch 3: \n",
      "Loss:     0.2760 Accuracy: 0.944186\n",
      "Epoch 71, Batch 4: \n",
      "Loss:     0.3296 Accuracy: 0.953488\n",
      "Epoch Number: 72\n",
      "Epoch 72, Batch 0: \n",
      "Loss:     0.3584 Accuracy: 0.958140\n",
      "Epoch 72, Batch 1: \n",
      "Loss:     0.4186 Accuracy: 0.930233\n",
      "Epoch 72, Batch 2: \n",
      "Loss:     0.2927 Accuracy: 0.920930\n",
      "Epoch 72, Batch 3: \n",
      "Loss:     0.3178 Accuracy: 0.930233\n",
      "Epoch 72, Batch 4: \n",
      "Loss:     0.3110 Accuracy: 0.934884\n",
      "Epoch Number: 73\n",
      "Epoch 73, Batch 0: \n",
      "Loss:     0.4884 Accuracy: 0.939535\n",
      "Epoch 73, Batch 1: \n",
      "Loss:     0.3680 Accuracy: 0.939535\n",
      "Epoch 73, Batch 2: \n",
      "Loss:     0.2346 Accuracy: 0.958140\n",
      "Epoch 73, Batch 3: \n",
      "Loss:     0.2364 Accuracy: 0.953488\n",
      "Epoch 73, Batch 4: \n",
      "Loss:     0.2710 Accuracy: 0.948837\n",
      "Epoch Number: 74\n",
      "Epoch 74, Batch 0: \n",
      "Loss:     0.2804 Accuracy: 0.958140\n",
      "Epoch 74, Batch 1: \n",
      "Loss:     0.2529 Accuracy: 0.953488\n",
      "Epoch 74, Batch 2: \n",
      "Loss:     0.2099 Accuracy: 0.953488\n",
      "Epoch 74, Batch 3: \n",
      "Loss:     0.2424 Accuracy: 0.958140\n",
      "Epoch 74, Batch 4: \n",
      "Loss:     0.2462 Accuracy: 0.939535\n",
      "Epoch Number: 75\n",
      "Epoch 75, Batch 0: \n",
      "Loss:     0.3696 Accuracy: 0.948837\n",
      "Epoch 75, Batch 1: \n",
      "Loss:     0.3152 Accuracy: 0.920930\n",
      "Epoch 75, Batch 2: \n",
      "Loss:     0.2246 Accuracy: 0.953488\n",
      "Epoch 75, Batch 3: \n",
      "Loss:     0.2601 Accuracy: 0.934884\n",
      "Epoch 75, Batch 4: \n",
      "Loss:     0.2776 Accuracy: 0.944186\n",
      "Epoch Number: 76\n",
      "Epoch 76, Batch 0: \n",
      "Loss:     0.3021 Accuracy: 0.962791\n",
      "Epoch 76, Batch 1: \n",
      "Loss:     0.3420 Accuracy: 0.967442\n",
      "Epoch 76, Batch 2: \n",
      "Loss:     0.3697 Accuracy: 0.953488\n",
      "Epoch 76, Batch 3: \n",
      "Loss:     0.3339 Accuracy: 0.948837\n",
      "Epoch 76, Batch 4: \n",
      "Loss:     0.3678 Accuracy: 0.948837\n",
      "Epoch Number: 77\n",
      "Epoch 77, Batch 0: \n",
      "Loss:     0.5419 Accuracy: 0.911628\n",
      "Epoch 77, Batch 1: \n",
      "Loss:     0.4337 Accuracy: 0.934884\n",
      "Epoch 77, Batch 2: \n",
      "Loss:     0.3589 Accuracy: 0.939535\n",
      "Epoch 77, Batch 3: \n",
      "Loss:     0.4386 Accuracy: 0.944186\n",
      "Epoch 77, Batch 4: \n",
      "Loss:     0.5043 Accuracy: 0.920930\n",
      "Epoch Number: 78\n",
      "Epoch 78, Batch 0: \n",
      "Loss:     0.5063 Accuracy: 0.939535\n",
      "Epoch 78, Batch 1: \n",
      "Loss:     0.4255 Accuracy: 0.939535\n",
      "Epoch 78, Batch 2: \n",
      "Loss:     0.3956 Accuracy: 0.934884\n",
      "Epoch 78, Batch 3: \n",
      "Loss:     0.3178 Accuracy: 0.962791\n",
      "Epoch 78, Batch 4: \n",
      "Loss:     0.4245 Accuracy: 0.948837\n",
      "Epoch Number: 79\n",
      "Epoch 79, Batch 0: \n",
      "Loss:     0.4694 Accuracy: 0.939535\n",
      "Epoch 79, Batch 1: \n",
      "Loss:     0.5149 Accuracy: 0.944186\n",
      "Epoch 79, Batch 2: \n",
      "Loss:     0.3716 Accuracy: 0.948837\n",
      "Epoch 79, Batch 3: \n",
      "Loss:     0.3513 Accuracy: 0.948837\n",
      "Epoch 79, Batch 4: \n",
      "Loss:     0.5002 Accuracy: 0.920930\n",
      "Epoch Number: 80\n",
      "Epoch 80, Batch 0: \n",
      "Loss:     0.3704 Accuracy: 0.948837\n",
      "Epoch 80, Batch 1: \n",
      "Loss:     0.4132 Accuracy: 0.948837\n",
      "Epoch 80, Batch 2: \n",
      "Loss:     0.4380 Accuracy: 0.948837\n",
      "Epoch 80, Batch 3: \n",
      "Loss:     0.4981 Accuracy: 0.930233\n",
      "Epoch 80, Batch 4: \n",
      "Loss:     0.3605 Accuracy: 0.958140\n",
      "Epoch Number: 81\n",
      "Epoch 81, Batch 0: \n",
      "Loss:     0.2695 Accuracy: 0.944186\n",
      "Epoch 81, Batch 1: \n",
      "Loss:     0.2840 Accuracy: 0.953488\n",
      "Epoch 81, Batch 2: \n",
      "Loss:     0.3014 Accuracy: 0.944186\n",
      "Epoch 81, Batch 3: \n",
      "Loss:     0.3232 Accuracy: 0.939535\n",
      "Epoch 81, Batch 4: \n",
      "Loss:     0.3901 Accuracy: 0.930233\n",
      "Epoch Number: 82\n",
      "Epoch 82, Batch 0: \n",
      "Loss:     0.4340 Accuracy: 0.944186\n",
      "Epoch 82, Batch 1: \n",
      "Loss:     0.3872 Accuracy: 0.958140\n",
      "Epoch 82, Batch 2: \n",
      "Loss:     0.3748 Accuracy: 0.962791\n",
      "Epoch 82, Batch 3: \n",
      "Loss:     0.3995 Accuracy: 0.967442\n",
      "Epoch 82, Batch 4: \n",
      "Loss:     0.5296 Accuracy: 0.958140\n",
      "Epoch Number: 83\n",
      "Epoch 83, Batch 0: \n",
      "Loss:     1.0387 Accuracy: 0.851163\n",
      "Epoch 83, Batch 1: \n",
      "Loss:     0.2581 Accuracy: 0.925581\n",
      "Epoch 83, Batch 2: \n",
      "Loss:     0.2639 Accuracy: 0.944186\n",
      "Epoch 83, Batch 3: \n",
      "Loss:     0.3061 Accuracy: 0.948837\n",
      "Epoch 83, Batch 4: \n",
      "Loss:     0.3497 Accuracy: 0.939535\n",
      "Epoch Number: 84\n",
      "Epoch 84, Batch 0: \n",
      "Loss:     1.0478 Accuracy: 0.818605\n",
      "Epoch 84, Batch 1: \n",
      "Loss:     0.4734 Accuracy: 0.851163\n",
      "Epoch 84, Batch 2: \n",
      "Loss:     0.3150 Accuracy: 0.897674\n",
      "Epoch 84, Batch 3: \n",
      "Loss:     0.2625 Accuracy: 0.906977\n",
      "Epoch 84, Batch 4: \n",
      "Loss:     0.5208 Accuracy: 0.916279\n",
      "Epoch Number: 85\n",
      "Epoch 85, Batch 0: \n",
      "Loss:     0.3538 Accuracy: 0.934884\n",
      "Epoch 85, Batch 1: \n",
      "Loss:     0.3365 Accuracy: 0.944186\n",
      "Epoch 85, Batch 2: \n",
      "Loss:     0.2561 Accuracy: 0.948837\n",
      "Epoch 85, Batch 3: \n",
      "Loss:     0.3515 Accuracy: 0.930233\n",
      "Epoch 85, Batch 4: \n",
      "Loss:     0.3775 Accuracy: 0.934884\n",
      "Epoch Number: 86\n",
      "Epoch 86, Batch 0: \n",
      "Loss:     0.4413 Accuracy: 0.930233\n",
      "Epoch 86, Batch 1: \n",
      "Loss:     0.3733 Accuracy: 0.948837\n",
      "Epoch 86, Batch 2: \n",
      "Loss:     0.4337 Accuracy: 0.967442\n",
      "Epoch 86, Batch 3: \n",
      "Loss:     0.4591 Accuracy: 0.967442\n",
      "Epoch 86, Batch 4: \n",
      "Loss:     0.4240 Accuracy: 0.944186\n",
      "Epoch Number: 87\n",
      "Epoch 87, Batch 0: \n",
      "Loss:     0.5933 Accuracy: 0.930233\n",
      "Epoch 87, Batch 1: \n",
      "Loss:     0.4057 Accuracy: 0.920930\n",
      "Epoch 87, Batch 2: \n",
      "Loss:     0.2050 Accuracy: 0.930233\n",
      "Epoch 87, Batch 3: \n",
      "Loss:     0.2016 Accuracy: 0.967442\n",
      "Epoch 87, Batch 4: \n",
      "Loss:     0.2541 Accuracy: 0.953488\n",
      "Epoch Number: 88\n",
      "Epoch 88, Batch 0: \n",
      "Loss:     0.2014 Accuracy: 0.958140\n",
      "Epoch 88, Batch 1: \n",
      "Loss:     0.2385 Accuracy: 0.953488\n",
      "Epoch 88, Batch 2: \n",
      "Loss:     0.2634 Accuracy: 0.958140\n",
      "Epoch 88, Batch 3: \n",
      "Loss:     0.3832 Accuracy: 0.948837\n",
      "Epoch 88, Batch 4: \n",
      "Loss:     0.4510 Accuracy: 0.948837\n",
      "Epoch Number: 89\n",
      "Epoch 89, Batch 0: \n",
      "Loss:     0.3482 Accuracy: 0.967442\n",
      "Epoch 89, Batch 1: \n",
      "Loss:     0.3461 Accuracy: 0.972093\n",
      "Epoch 89, Batch 2: \n",
      "Loss:     0.3486 Accuracy: 0.958140\n",
      "Epoch 89, Batch 3: \n",
      "Loss:     0.4382 Accuracy: 0.962791\n",
      "Epoch 89, Batch 4: \n",
      "Loss:     0.4426 Accuracy: 0.953488\n",
      "Epoch Number: 90\n",
      "Epoch 90, Batch 0: \n",
      "Loss:     0.4304 Accuracy: 0.953488\n",
      "Epoch 90, Batch 1: \n",
      "Loss:     0.4155 Accuracy: 0.962791\n",
      "Epoch 90, Batch 2: \n",
      "Loss:     0.4208 Accuracy: 0.948837\n",
      "Epoch 90, Batch 3: \n",
      "Loss:     0.4959 Accuracy: 0.962791\n",
      "Epoch 90, Batch 4: \n",
      "Loss:     0.5171 Accuracy: 0.962791\n",
      "Epoch Number: 91\n",
      "Epoch 91, Batch 0: \n",
      "Loss:     0.5254 Accuracy: 0.962791\n",
      "Epoch 91, Batch 1: \n",
      "Loss:     0.5230 Accuracy: 0.967442\n",
      "Epoch 91, Batch 2: \n",
      "Loss:     0.4986 Accuracy: 0.953488\n",
      "Epoch 91, Batch 3: \n",
      "Loss:     0.5108 Accuracy: 0.953488\n",
      "Epoch 91, Batch 4: \n",
      "Loss:     0.5131 Accuracy: 0.953488\n",
      "Epoch Number: 92\n",
      "Epoch 92, Batch 0: \n",
      "Loss:     0.5284 Accuracy: 0.962791\n",
      "Epoch 92, Batch 1: \n",
      "Loss:     0.5599 Accuracy: 0.958140\n",
      "Epoch 92, Batch 2: \n",
      "Loss:     0.5934 Accuracy: 0.953488\n",
      "Epoch 92, Batch 3: \n",
      "Loss:     0.6106 Accuracy: 0.953488\n",
      "Epoch 92, Batch 4: \n",
      "Loss:     0.9552 Accuracy: 0.944186\n",
      "Epoch Number: 93\n",
      "Epoch 93, Batch 0: \n",
      "Loss:     1.1987 Accuracy: 0.827907\n",
      "Epoch 93, Batch 1: \n",
      "Loss:     0.4372 Accuracy: 0.851163\n",
      "Epoch 93, Batch 2: \n",
      "Loss:     0.2480 Accuracy: 0.930233\n",
      "Epoch 93, Batch 3: \n",
      "Loss:     0.3106 Accuracy: 0.934884\n",
      "Epoch 93, Batch 4: \n",
      "Loss:     0.3252 Accuracy: 0.948837\n",
      "Epoch Number: 94\n",
      "Epoch 94, Batch 0: \n",
      "Loss:     0.5682 Accuracy: 0.888372\n",
      "Epoch 94, Batch 1: \n",
      "Loss:     0.5111 Accuracy: 0.920930\n",
      "Epoch 94, Batch 2: \n",
      "Loss:     0.3057 Accuracy: 0.939535\n",
      "Epoch 94, Batch 3: \n",
      "Loss:     0.4308 Accuracy: 0.930233\n",
      "Epoch 94, Batch 4: \n",
      "Loss:     0.3245 Accuracy: 0.906977\n",
      "Epoch Number: 95\n",
      "Epoch 95, Batch 0: \n",
      "Loss:     0.2151 Accuracy: 0.944186\n",
      "Epoch 95, Batch 1: \n",
      "Loss:     0.2173 Accuracy: 0.958140\n",
      "Epoch 95, Batch 2: \n",
      "Loss:     0.2835 Accuracy: 0.944186\n",
      "Epoch 95, Batch 3: \n",
      "Loss:     0.4240 Accuracy: 0.934884\n",
      "Epoch 95, Batch 4: \n",
      "Loss:     0.4927 Accuracy: 0.939535\n",
      "Epoch Number: 96\n",
      "Epoch 96, Batch 0: \n",
      "Loss:     0.2514 Accuracy: 0.962791\n",
      "Epoch 96, Batch 1: \n",
      "Loss:     0.2909 Accuracy: 0.967442\n",
      "Epoch 96, Batch 2: \n",
      "Loss:     0.3441 Accuracy: 0.958140\n",
      "Epoch 96, Batch 3: \n",
      "Loss:     0.4875 Accuracy: 0.953488\n",
      "Epoch 96, Batch 4: \n",
      "Loss:     0.6379 Accuracy: 0.958140\n",
      "Epoch Number: 97\n",
      "Epoch 97, Batch 0: \n",
      "Loss:     0.8955 Accuracy: 0.939535\n",
      "Epoch 97, Batch 1: \n",
      "Loss:     0.6847 Accuracy: 0.948837\n",
      "Epoch 97, Batch 2: \n",
      "Loss:     0.3208 Accuracy: 0.916279\n",
      "Epoch 97, Batch 3: \n",
      "Loss:     0.5979 Accuracy: 0.939535\n",
      "Epoch 97, Batch 4: \n",
      "Loss:     0.5437 Accuracy: 0.925581\n",
      "Epoch Number: 98\n",
      "Epoch 98, Batch 0: \n",
      "Loss:     0.7126 Accuracy: 0.888372\n",
      "Epoch 98, Batch 1: \n",
      "Loss:     0.3424 Accuracy: 0.897674\n",
      "Epoch 98, Batch 2: \n",
      "Loss:     0.1775 Accuracy: 0.944186\n",
      "Epoch 98, Batch 3: \n",
      "Loss:     0.1991 Accuracy: 0.972093\n",
      "Epoch 98, Batch 4: \n",
      "Loss:     0.2128 Accuracy: 0.972093\n",
      "Epoch Number: 99\n",
      "Epoch 99, Batch 0: \n",
      "Loss:     0.3904 Accuracy: 0.958140\n",
      "Epoch 99, Batch 1: \n",
      "Loss:     0.2812 Accuracy: 0.934884\n",
      "Epoch 99, Batch 2: \n",
      "Loss:     0.2483 Accuracy: 0.953488\n",
      "Epoch 99, Batch 3: \n",
      "Loss:     0.3509 Accuracy: 0.972093\n",
      "Epoch 99, Batch 4: \n",
      "Loss:     0.3626 Accuracy: 0.958140\n",
      "Epoch Number: 100\n",
      "Epoch 100, Batch 0: \n",
      "Loss:     0.3177 Accuracy: 0.962791\n",
      "Epoch 100, Batch 1: \n",
      "Loss:     0.2636 Accuracy: 0.967442\n",
      "Epoch 100, Batch 2: \n",
      "Loss:     0.2646 Accuracy: 0.939535\n",
      "Epoch 100, Batch 3: \n",
      "Loss:     0.3276 Accuracy: 0.962791\n",
      "Epoch 100, Batch 4: \n",
      "Loss:     0.3588 Accuracy: 0.962791\n",
      "Epoch Number: 101\n",
      "Epoch 101, Batch 0: \n",
      "Loss:     0.3759 Accuracy: 0.953488\n",
      "Epoch 101, Batch 1: \n",
      "Loss:     0.3953 Accuracy: 0.958140\n",
      "Epoch 101, Batch 2: \n",
      "Loss:     0.4636 Accuracy: 0.916279\n",
      "Epoch 101, Batch 3: \n",
      "Loss:     0.3216 Accuracy: 0.934884\n",
      "Epoch 101, Batch 4: \n",
      "Loss:     0.2991 Accuracy: 0.939535\n",
      "Epoch Number: 102\n",
      "Epoch 102, Batch 0: \n",
      "Loss:     0.4198 Accuracy: 0.897674\n",
      "Epoch 102, Batch 1: \n",
      "Loss:     0.2184 Accuracy: 0.920930\n",
      "Epoch 102, Batch 2: \n",
      "Loss:     0.2398 Accuracy: 0.962791\n",
      "Epoch 102, Batch 3: \n",
      "Loss:     0.4082 Accuracy: 0.846512\n",
      "Epoch 102, Batch 4: \n",
      "Loss:     0.3731 Accuracy: 0.851163\n",
      "Epoch Number: 103\n",
      "Epoch 103, Batch 0: \n",
      "Loss:     0.3006 Accuracy: 0.860465\n",
      "Epoch 103, Batch 1: \n",
      "Loss:     0.2645 Accuracy: 0.888372\n",
      "Epoch 103, Batch 2: \n",
      "Loss:     0.2641 Accuracy: 0.902326\n",
      "Epoch 103, Batch 3: \n",
      "Loss:     0.3380 Accuracy: 0.860465\n",
      "Epoch 103, Batch 4: \n",
      "Loss:     0.2573 Accuracy: 0.925581\n",
      "Epoch Number: 104\n",
      "Epoch 104, Batch 0: \n",
      "Loss:     0.2940 Accuracy: 0.911628\n",
      "Epoch 104, Batch 1: \n",
      "Loss:     0.1744 Accuracy: 0.948837\n",
      "Epoch 104, Batch 2: \n",
      "Loss:     0.2012 Accuracy: 0.930233\n",
      "Epoch 104, Batch 3: \n",
      "Loss:     0.2573 Accuracy: 0.930233\n",
      "Epoch 104, Batch 4: \n",
      "Loss:     0.2727 Accuracy: 0.925581\n",
      "Epoch Number: 105\n",
      "Epoch 105, Batch 0: \n",
      "Loss:     0.2138 Accuracy: 0.944186\n",
      "Epoch 105, Batch 1: \n",
      "Loss:     0.2682 Accuracy: 0.916279\n",
      "Epoch 105, Batch 2: \n",
      "Loss:     0.2086 Accuracy: 0.948837\n",
      "Epoch 105, Batch 3: \n",
      "Loss:     0.2148 Accuracy: 0.925581\n",
      "Epoch 105, Batch 4: \n",
      "Loss:     0.2636 Accuracy: 0.948837\n",
      "Epoch Number: 106\n",
      "Epoch 106, Batch 0: \n",
      "Loss:     0.2318 Accuracy: 0.948837\n",
      "Epoch 106, Batch 1: \n",
      "Loss:     0.2394 Accuracy: 0.962791\n",
      "Epoch 106, Batch 2: \n",
      "Loss:     0.2278 Accuracy: 0.953488\n",
      "Epoch 106, Batch 3: \n",
      "Loss:     0.2433 Accuracy: 0.948837\n",
      "Epoch 106, Batch 4: \n",
      "Loss:     0.2421 Accuracy: 0.967442\n",
      "Epoch Number: 107\n",
      "Epoch 107, Batch 0: \n",
      "Loss:     0.2098 Accuracy: 0.958140\n",
      "Epoch 107, Batch 1: \n",
      "Loss:     0.2204 Accuracy: 0.958140\n",
      "Epoch 107, Batch 2: \n",
      "Loss:     0.2671 Accuracy: 0.948837\n",
      "Epoch 107, Batch 3: \n",
      "Loss:     0.2421 Accuracy: 0.962791\n",
      "Epoch 107, Batch 4: \n",
      "Loss:     0.2410 Accuracy: 0.962791\n",
      "Epoch Number: 108\n",
      "Epoch 108, Batch 0: \n",
      "Loss:     0.2529 Accuracy: 0.958140\n",
      "Epoch 108, Batch 1: \n",
      "Loss:     0.2421 Accuracy: 0.962791\n",
      "Epoch 108, Batch 2: \n",
      "Loss:     0.3023 Accuracy: 0.953488\n",
      "Epoch 108, Batch 3: \n",
      "Loss:     0.3101 Accuracy: 0.958140\n",
      "Epoch 108, Batch 4: \n",
      "Loss:     0.3190 Accuracy: 0.953488\n",
      "Epoch Number: 109\n",
      "Epoch 109, Batch 0: \n",
      "Loss:     0.3221 Accuracy: 0.958140\n",
      "Epoch 109, Batch 1: \n",
      "Loss:     0.3139 Accuracy: 0.953488\n",
      "Epoch 109, Batch 2: \n",
      "Loss:     0.3493 Accuracy: 0.948837\n",
      "Epoch 109, Batch 3: \n",
      "Loss:     0.3526 Accuracy: 0.953488\n",
      "Epoch 109, Batch 4: \n",
      "Loss:     0.3629 Accuracy: 0.953488\n",
      "Epoch Number: 110\n",
      "Epoch 110, Batch 0: \n",
      "Loss:     0.3705 Accuracy: 0.948837\n",
      "Epoch 110, Batch 1: \n",
      "Loss:     0.3626 Accuracy: 0.958140\n",
      "Epoch 110, Batch 2: \n",
      "Loss:     0.3830 Accuracy: 0.958140\n",
      "Epoch 110, Batch 3: \n",
      "Loss:     0.3906 Accuracy: 0.953488\n",
      "Epoch 110, Batch 4: \n",
      "Loss:     0.3994 Accuracy: 0.953488\n",
      "Epoch Number: 111\n",
      "Epoch 111, Batch 0: \n",
      "Loss:     0.4134 Accuracy: 0.948837\n",
      "Epoch 111, Batch 1: \n",
      "Loss:     0.4006 Accuracy: 0.953488\n",
      "Epoch 111, Batch 2: \n",
      "Loss:     0.4095 Accuracy: 0.953488\n",
      "Epoch 111, Batch 3: \n",
      "Loss:     0.4152 Accuracy: 0.953488\n",
      "Epoch 111, Batch 4: \n",
      "Loss:     0.4188 Accuracy: 0.953488\n",
      "Epoch Number: 112\n",
      "Epoch 112, Batch 0: \n",
      "Loss:     0.4688 Accuracy: 0.958140\n",
      "Epoch 112, Batch 1: \n",
      "Loss:     0.4816 Accuracy: 0.953488\n",
      "Epoch 112, Batch 2: \n",
      "Loss:     0.5278 Accuracy: 0.944186\n",
      "Epoch 112, Batch 3: \n",
      "Loss:     0.3786 Accuracy: 0.893023\n",
      "Epoch 112, Batch 4: \n",
      "Loss:     0.2498 Accuracy: 0.944186\n",
      "Epoch Number: 113\n",
      "Epoch 113, Batch 0: \n",
      "Loss:     0.4262 Accuracy: 0.916279\n",
      "Epoch 113, Batch 1: \n",
      "Loss:     0.3614 Accuracy: 0.953488\n",
      "Epoch 113, Batch 2: \n",
      "Loss:     0.1884 Accuracy: 0.944186\n",
      "Epoch 113, Batch 3: \n",
      "Loss:     0.2071 Accuracy: 0.925581\n",
      "Epoch 113, Batch 4: \n",
      "Loss:     0.2024 Accuracy: 0.948837\n",
      "Epoch Number: 114\n",
      "Epoch 114, Batch 0: \n",
      "Loss:     0.2564 Accuracy: 0.944186\n",
      "Epoch 114, Batch 1: \n",
      "Loss:     0.3056 Accuracy: 0.934884\n",
      "Epoch 114, Batch 2: \n",
      "Loss:     0.3084 Accuracy: 0.934884\n",
      "Epoch 114, Batch 3: \n",
      "Loss:     0.4757 Accuracy: 0.934884\n",
      "Epoch 114, Batch 4: \n",
      "Loss:     0.4978 Accuracy: 0.953488\n",
      "Epoch Number: 115\n",
      "Epoch 115, Batch 0: \n",
      "Loss:     0.4161 Accuracy: 0.962791\n",
      "Epoch 115, Batch 1: \n",
      "Loss:     0.4019 Accuracy: 0.972093\n",
      "Epoch 115, Batch 2: \n",
      "Loss:     0.4803 Accuracy: 0.958140\n",
      "Epoch 115, Batch 3: \n",
      "Loss:     0.5085 Accuracy: 0.944186\n",
      "Epoch 115, Batch 4: \n",
      "Loss:     0.4995 Accuracy: 0.948837\n",
      "Epoch Number: 116\n",
      "Epoch 116, Batch 0: \n",
      "Loss:     0.4295 Accuracy: 0.958140\n",
      "Epoch 116, Batch 1: \n",
      "Loss:     0.4204 Accuracy: 0.967442\n",
      "Epoch 116, Batch 2: \n",
      "Loss:     0.4279 Accuracy: 0.967442\n",
      "Epoch 116, Batch 3: \n",
      "Loss:     0.4665 Accuracy: 0.958140\n",
      "Epoch 116, Batch 4: \n",
      "Loss:     0.4502 Accuracy: 0.962791\n",
      "Epoch Number: 117\n",
      "Epoch 117, Batch 0: \n",
      "Loss:     0.4936 Accuracy: 0.962791\n",
      "Epoch 117, Batch 1: \n",
      "Loss:     0.3634 Accuracy: 0.953488\n",
      "Epoch 117, Batch 2: \n",
      "Loss:     0.3484 Accuracy: 0.944186\n",
      "Epoch 117, Batch 3: \n",
      "Loss:     0.4102 Accuracy: 0.958140\n",
      "Epoch 117, Batch 4: \n",
      "Loss:     0.5322 Accuracy: 0.934884\n",
      "Epoch Number: 118\n",
      "Epoch 118, Batch 0: \n",
      "Loss:     0.6111 Accuracy: 0.925581\n",
      "Epoch 118, Batch 1: \n",
      "Loss:     0.4095 Accuracy: 0.948837\n",
      "Epoch 118, Batch 2: \n",
      "Loss:     0.4219 Accuracy: 0.953488\n",
      "Epoch 118, Batch 3: \n",
      "Loss:     0.4517 Accuracy: 0.953488\n",
      "Epoch 118, Batch 4: \n",
      "Loss:     0.5744 Accuracy: 0.934884\n",
      "Epoch Number: 119\n",
      "Epoch 119, Batch 0: \n",
      "Loss:     0.6023 Accuracy: 0.925581\n",
      "Epoch 119, Batch 1: \n",
      "Loss:     0.6317 Accuracy: 0.920930\n",
      "Epoch 119, Batch 2: \n",
      "Loss:     0.5149 Accuracy: 0.906977\n",
      "Epoch 119, Batch 3: \n",
      "Loss:     0.5892 Accuracy: 0.962791\n",
      "Epoch 119, Batch 4: \n",
      "Loss:     0.5051 Accuracy: 0.930233\n",
      "Epoch Number: 120\n",
      "Epoch 120, Batch 0: \n",
      "Loss:     0.4471 Accuracy: 0.925581\n",
      "Epoch 120, Batch 1: \n",
      "Loss:     0.4470 Accuracy: 0.939535\n",
      "Epoch 120, Batch 2: \n",
      "Loss:     0.4181 Accuracy: 0.944186\n",
      "Epoch 120, Batch 3: \n",
      "Loss:     0.4164 Accuracy: 0.944186\n",
      "Epoch 120, Batch 4: \n",
      "Loss:     0.4378 Accuracy: 0.939535\n",
      "Epoch Number: 121\n",
      "Epoch 121, Batch 0: \n",
      "Loss:     0.4410 Accuracy: 0.939535\n",
      "Epoch 121, Batch 1: \n",
      "Loss:     0.4360 Accuracy: 0.948837\n",
      "Epoch 121, Batch 2: \n",
      "Loss:     0.4516 Accuracy: 0.948837\n",
      "Epoch 121, Batch 3: \n",
      "Loss:     0.4498 Accuracy: 0.948837\n",
      "Epoch 121, Batch 4: \n",
      "Loss:     0.4531 Accuracy: 0.944186\n",
      "Epoch Number: 122\n",
      "Epoch 122, Batch 0: \n",
      "Loss:     0.4546 Accuracy: 0.944186\n",
      "Epoch 122, Batch 1: \n",
      "Loss:     0.4415 Accuracy: 0.948837\n",
      "Epoch 122, Batch 2: \n",
      "Loss:     0.4535 Accuracy: 0.948837\n",
      "Epoch 122, Batch 3: \n",
      "Loss:     0.4737 Accuracy: 0.948837\n",
      "Epoch 122, Batch 4: \n",
      "Loss:     0.4827 Accuracy: 0.944186\n",
      "Epoch Number: 123\n",
      "Epoch 123, Batch 0: \n",
      "Loss:     0.4876 Accuracy: 0.944186\n",
      "Epoch 123, Batch 1: \n",
      "Loss:     0.4900 Accuracy: 0.944186\n",
      "Epoch 123, Batch 2: \n",
      "Loss:     0.4946 Accuracy: 0.944186\n",
      "Epoch 123, Batch 3: \n",
      "Loss:     0.5013 Accuracy: 0.944186\n",
      "Epoch 123, Batch 4: \n",
      "Loss:     0.5070 Accuracy: 0.944186\n",
      "Epoch Number: 124\n",
      "Epoch 124, Batch 0: \n",
      "Loss:     0.5121 Accuracy: 0.944186\n",
      "Epoch 124, Batch 1: \n",
      "Loss:     0.5153 Accuracy: 0.944186\n",
      "Epoch 124, Batch 2: \n",
      "Loss:     0.5176 Accuracy: 0.944186\n",
      "Epoch 124, Batch 3: \n",
      "Loss:     0.5206 Accuracy: 0.944186\n",
      "Epoch 124, Batch 4: \n",
      "Loss:     0.5230 Accuracy: 0.944186\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Removes prior weights, biases, etc.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "# Create placeholders:\n",
    "    x = tf.placeholder(tf.float32, shape = (None, 3,256,256), name = \"x\")\n",
    "    y = tf.placeholder(tf.float32, shape = (None, 2), name = \"y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "\n",
    "\n",
    "    logits = conv_net(x, keep_prob)\n",
    "\n",
    "    # Name logits Tensor, so that is can be loaded from disk after training\n",
    "    logits = tf.identity(logits, name=\"logits\")\n",
    "\n",
    "    # Loss and Optimizer, using the sigmoid function for binary classification rather than softmax\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name= \"accuracy\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    with tf.Session() as sess:\n",
    "        # Initializing the variables\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        # Training cycle\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch Number: \" + str(epoch))\n",
    "            \n",
    "            for batch in range(5):\n",
    "                for batch_images, batch_labels, batch_paths in get_mini_batches(batch_size, train_images[batch], train_labels[batch], train_paths[batch]):\n",
    "                    c = list(zip(batch_images, batch_labels))\n",
    "                    random.shuffle(c)\n",
    "                    batch_images, batch_labels = zip(*c)\n",
    "                    train_neural_network(sess, optimizer, keep_probability, batch_images, batch_labels)\n",
    "\n",
    "                print(\"Epoch \" + str(epoch) + \", Batch \" + str(batch) + \": \")   \n",
    "                print_stats(sess, valid_images, valid_labels, cost, accuracy)\n",
    "            #learning_rate = learning_rate/1.05\n",
    "   \n",
    "                \n",
    "            \n",
    "        # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"./training_sess\")\n",
    "        \n",
    "print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tweaking the code and adjusting the hyperparameters, I've increased the accuracy from my prior submission by almost 20%. \n",
    "\n",
    "\n",
    "### Goal 3: Evaluate the neural network and show us the results\n",
    "#### Testing the Model:\n",
    "\n",
    "We will test the model using our test_images and test_labels data. We will do this by creating a method to test the model that will load the model with get_tensor_by_name and run it again our testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9241071428571429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_model_path = \"./training_sess\"\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    #test_images, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_image_batch, test_label_batch, test_path_batch in get_mini_batches(batch_size, test_images, test_labels, test_paths):\n",
    "            \n",
    "            transposed_images = np.array(test_image_batch).transpose(0, 3, 1, 2)\n",
    "            \n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: transposed_images, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_images, random_test_labels = tuple(zip(*random.sample(list(zip(test_images, test_labels)), n_samples)))\n",
    "        \n",
    "        \n",
    "        transposed_images = np.array(test_image_batch).transpose(0, 3, 1, 2)\n",
    "            \n",
    "            \n",
    "        \n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), 2),\n",
    "            feed_dict={loaded_x: transposed_images, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woot! This testing accuracy is 8% higher than my previous submission. \n",
    "\n",
    "Although I could not get display_image_predictions to function, it was intended to display a few random images and the model's associated probabilites for each class in a table format. However, looking at the testing accuracy, the model's prospects look encouraging. \n",
    "\n",
    "### Goal 4: Testing the Network on Local Machines:\n",
    "\n",
    "Running the following cells will allow you to input the file path of an image, and the model will provide you with a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sarahhernandez/Documents/4. Important Docs/Passio/tissues.jpg\n"
     ]
    }
   ],
   "source": [
    "# Enter filepath of image\n",
    "input_img = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Enter 0 if it's Not Food, 1 if it's Food\n",
    "input_label = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_label = []\n",
    "if input_label == str(0):\n",
    "    single_label = [1, 0]\n",
    "elif input_label == str(1):\n",
    "    single_label = [0, 1]\n",
    "else:\n",
    "    print(\"Invalid label input, please run above cell again\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(input_img)\n",
    "img = cv2.resize(img,(256,256))\n",
    "img = normalize(img)\n",
    "img_stack = [img]\n",
    "label_stack = [single_label]\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "       \n",
    "        transposed_images = np.array(img_stack).transpose(0, 3, 1, 2)\n",
    "            \n",
    "        test_batch_acc_total += sess.run(loaded_acc, feed_dict={loaded_x: transposed_images, loaded_y: label_stack, loaded_keep_prob: 1.0})\n",
    "        \n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! If the testing accuracy is 1, then the model correctly predicted the given image, if the testing accuracy is 0, then the model failed to predict if the image was food or not food. \n",
    "\n",
    "\n",
    "#### Future Work\n",
    "\n",
    "For further improvements to the program, I would implement the following:\n",
    "\n",
    "* I would plot the training and validation accuracies vs. epochs to check for under and overfitting. \n",
    "* I would further expand the dataset by randomly cropping, translating and scaling images, or by using an online dataset\n",
    "* I would implement early stopping to prevent overfitting \n",
    "* I'd like to see the results of implementing a k-fold cross validation, as it's good to use with limited datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
